{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview: Single-Model Tuning Pipeline\n",
    "\n",
    "- This notebook outlines the development workflow of a **Random Forestâ€“based machine learning pipeline** for iron deficiency identification. It includes the full process of training, feature selection, and hyperparameter tuning.\n",
    "\n",
    "- Due to internal data governance policies, we are unable to share the original patient-level datasets. As a result, this notebook is **not executable out-of-the-box**. This notebook is intended to illustrate the overall modeling workflow and the logic of each step to support transparency and potential reproducibility when appropriate data access is granted.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Workflow Summary\n",
    "\n",
    "1. **Import and Environment Setup**  \n",
    "   Load all required libraries and project paths.\n",
    "\n",
    "2. **Data Preparation**  \n",
    "   Paths are initialized to read in pre-split training and test datasets for internal and external validation (An-Nan Hospital and Wei-Gong Memorial Hospital).\n",
    "\n",
    "3. **Cross-Validation with a Single Model (Random Forest)**  \n",
    "   Perform stratified K-fold training on a fixed variable combination, storing fold-wise predictions and metrics.\n",
    "\n",
    "4. **Fold Selection**  \n",
    "   Identify the best-performing fold based on AUROC and other metrics for subsequent feature analysis.\n",
    "\n",
    "5. **Feature Importance Extraction**  \n",
    "   Use the selected Random Forest model to compute importance scores and sort features accordingly.\n",
    "\n",
    "6. **Recursive Feature Elimination**  \n",
    "   Iteratively remove the least important features to evaluate performance impact and identify the optimal feature subset.\n",
    "\n",
    "7. **Model Retraining with Top Features**  \n",
    "   Re-train the model using only the best N features determined in the previous step.\n",
    "\n",
    "8. **Hyperparameter Tuning**  \n",
    "   Perform `RandomizedSearchCV` to optimize Random Forest hyperparameters for better performance\n",
    "\n",
    "9. **Final Evaluation**  \n",
    "   Train the final model using optimized settings and selected features, and evaluate performance across all cohorts using AUROC, AUPRC, and Brier score.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# ============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import dump, load\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, \n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# ============================\n",
    "\n",
    "sys.path.append('../src') \n",
    "from config import data_dir, cache_dir\n",
    "from model_utils import (\n",
    "    get_chosen_columns,\n",
    "    evaluate_model,\n",
    "    imputer,\n",
    "    scaling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 190   \n",
    "threshold= 0.5   # for binary threshold for positive prediction\n",
    "K=5              # for k-fold cross validation\n",
    "\n",
    "female_ferritin_threshold= 11\n",
    "male_ferritin_threshold= 23.9\n",
    "TIBC_threshold= 450\n",
    "\n",
    "weight_for_0 = 1 # for iron deficiency negative samples\n",
    "weight_for_1 = 8 # for iron deficiency positive samples\n",
    "    \n",
    "impute_method= 'zero'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data (internal validation) \n",
    "data_train = pd.read_csv(\n",
    "    os.path.join(data_dir, 'CMUH_train_set.csv'),\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "data_test = pd.read_csv(\n",
    "    os.path.join(data_dir, 'CMUH_valid_set.csv'),\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "# Load external validation datasets from AN and WMH hospitals\n",
    "data_ANH = pd.read_csv(\n",
    "    os.path.join(data_dir, 'ANH.csv'),\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "data_WMH = pd.read_csv(\n",
    "    os.path.join(data_dir, 'WMH.csv'),\n",
    "    index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train: 18.83% (1026 positive cases, 4423 negative cases)\n",
      "data_test: 15.99% (273 positive cases, 1434 negative cases)\n",
      "data_AN: 19.27% (264 positive cases, 1106 negative cases)\n",
      "data_WK: 19.62% (249 positive cases, 1020 negative cases)\n"
     ]
    }
   ],
   "source": [
    "# Recalculate and summarize the proportion and counts of positive cases in each dataset\n",
    "positive_ratios_counts = {\n",
    "    'data_train': (data_train['Label'].mean() * 100, data_train['Label'].sum(), len(data_train) - data_train['Label'].sum()),\n",
    "    'data_test': (data_test['Label'].mean() * 100, data_test['Label'].sum(), len(data_test) - data_test['Label'].sum()),\n",
    "    'data_ANH': (data_ANH['Label'].mean() * 100, data_ANH['Label'].sum(), len(data_ANH) - data_ANH['Label'].sum()),\n",
    "    'data_WMH': (data_WMH['Label'].mean() * 100, data_WMH['Label'].sum(), len(data_WMH) - data_WMH['Label'].sum()),\n",
    "}\n",
    "\n",
    "# Display the results: percentage of positives, and counts of positive/negative cases\n",
    "for dataset, (percentage, positive, negative) in positive_ratios_counts.items():\n",
    "    print(f\"{dataset}: {percentage:.2f}% ({int(positive)} positive cases, {int(negative)} negative cases)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross validation with single model**\n",
    "- model: Random forest\n",
    "- Impute method: Zero\n",
    "- variables set: 'CBC','CPD','basic'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups and retrieve selected columns\n",
    "var = ['CBC', 'CPD', 'basic']\n",
    "chosen_col = get_chosen_columns(var)\n",
    "\n",
    "# Define imputation strategy (only testing 'zero' imputation here)\n",
    "impute_groups = ['zero']\n",
    "\n",
    "# Assign features and labels from datasets\n",
    "x = data_train.loc[:, chosen_col] \n",
    "y = data_train['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cross-validation splitter\n",
    "skf = StratifiedKFold(n_splits=K, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Initialize storage for performance metrics\n",
    "metrics_columns = ['AUROC', 'AUPRC', 'accuracy', 'F1_score', 'recall', 'specificity', 'precision', 'NPV']\n",
    "outcome_total = pd.DataFrame({col: pd.Series(dtype='float') for col in metrics_columns})\n",
    "\n",
    "# Begin cross-validation loop\n",
    "fold = 0\n",
    "for train_index, val_index in skf.split(x, y):\n",
    "    fold += 1\n",
    "    \n",
    "    # Save indices to file for reproducibility\n",
    "    dump(train_index, f'{cache_dir}\\\\train_index_{fold}.joblib')\n",
    "    dump(val_index, f'{cache_dir}\\\\val_index_{fold}.joblib')\n",
    "\n",
    "    # Split data into training and validation\n",
    "    x_train_cv, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Compute sample weights for training\n",
    "    weights_train = np.array([weight_for_0 if label == 0 else weight_for_1 for label in y_train_cv])\n",
    "\n",
    "    # Imputation and scaling (train and apply same to val/test/external sets)\n",
    "    x_train_imp, m_mean = imputer(x_train_cv, impute_method, train=True)\n",
    "    x_train_scale, train_scaler = scaling(x_train_imp, None, train=True)\n",
    "\n",
    "    x_val_imp = imputer(x_val, impute_method, False, m_mean)\n",
    "    x_val_scale, _ = scaling(x_val_imp, train_scaler, train=False)\n",
    "\n",
    "    # Train random forest model\n",
    "    model = RandomForestClassifier(random_state=random_seed)\n",
    "    model.fit(x_train_scale, y_train_cv, sample_weight=weights_train)\n",
    "\n",
    "    # Inference for all datasets\n",
    "    prob_val = model.predict_proba(x_val_scale)[:, 1]\n",
    "\n",
    "    # Evaluate performance\n",
    "    con_matrix_val, outcome, pred = evaluate_model(prob_val, y_val, threshold)\n",
    "\n",
    "    # Append fold performance results\n",
    "    outcome_total = pd.concat([outcome_total, outcome], ignore_index=True)\n",
    "\n",
    "# Print full performance tables\n",
    "print(outcome_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pick up the Best-Performing Cross-Validation Fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific fold index according to result in training set\n",
    "i = 3  \n",
    "print(var)\n",
    "\n",
    "# Define target variable\n",
    "y = data_train['Label']\n",
    "\n",
    "# Extract features for all datasets\n",
    "x = data_train.loc[:, chosen_col] \n",
    "\n",
    "# Load saved indices for the selected fold\n",
    "train_index = load(f'{cache_dir}\\\\train_index_{i}.joblib')\n",
    "val_index = load(f'{cache_dir}\\\\val_index_{i}.joblib')\n",
    "\n",
    "# Split into training and validation sets\n",
    "x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "# Imputation and scaling\n",
    "x_train_imp, m_mean = imputer(x_train, impute_method, train=True)\n",
    "x_train_scale, train_scaler = scaling(x_train_imp, None, train=True)\n",
    "\n",
    "x_val_imp = imputer(x_val, impute_method, False, m_mean)\n",
    "x_val_scale, _ = scaling(x_val_imp, train_scaler, train=False)\n",
    "\n",
    "# Compute sample weights for training\n",
    "weights_train = np.array([weight_for_0 if label == 0 else weight_for_1 for label in y_train])\n",
    "\n",
    "# Train model on selected fold\n",
    "model = RandomForestClassifier(random_state=random_seed)\n",
    "model.fit(x_train_scale, y_train, sample_weight=weights_train)\n",
    "\n",
    "# Predict probabilities\n",
    "prob_val = model.predict_proba(x_val_scale)[:, 1]\n",
    "\n",
    "# Evaluate model performance\n",
    "con_matrix_val, outcome, pred = evaluate_model(prob_val, y_val, threshold)\n",
    "\n",
    "# Print results for this fold\n",
    "print(outcome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Importances from Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the trained model\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get indices of the top 20 most important features (from lowest to highest)\n",
    "indices = np.argsort(importances)[-20:]\n",
    "\n",
    "# Retrieve the original feature names\n",
    "features = x.columns\n",
    "\n",
    "# Set up the plot size\n",
    "plt.subplots(figsize=(3, 7))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recursive Feature Elimination Based on Importance Ranking**\n",
    "- This section performs backward feature elimination using the importance rankings derived from the random forest model. Starting from all features, we iteratively remove one feature at a time (from least to most important) and retrain the model to evaluate how performance changes on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained model\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)  # Sort features by importance (low to high)\n",
    "\n",
    "dump(importances, rf'{cache_dir}\\importances_{var}.joblib')\n",
    "dump(indices, rf'{cache_dir}\\index_{var}.joblib')\n",
    "\n",
    "features = x.columns.tolist()\n",
    "feature_sel = [features[i] for i in indices]\n",
    "features_selected = ['0'] + feature_sel\n",
    "\n",
    "perf_list = []\n",
    "\n",
    "# Begin backward elimination loop\n",
    "# Each iteration removes one feature (starting from the least important), retrains the model, and evaluates performance\n",
    "for i in tqdm(features_selected[:-1]): \n",
    "    if i == '0':\n",
    "        # Initial condition: full feature set\n",
    "        features_selected = feature_sel.copy()\n",
    "    else:\n",
    "        # Remove the current feature from selection\n",
    "        features_selected.remove(i)\n",
    "    \n",
    "    # Subset the selected features for training and validation\n",
    "    x_train_sel = x_train[features_selected]\n",
    "    x_val_sel = x_val[features_selected]\n",
    "\n",
    "    x_train_imp, m_mean = imputer(x_train_sel, impute_method, train=True)\n",
    "    x_train_scale, train_scaler = scaling(x_train_imp, None, train=True)\n",
    "\n",
    "    x_val_imp = imputer(x_val_sel, impute_method, False, m_mean)\n",
    "    x_val_scale, _ = scaling(x_val_imp, train_scaler, train=False)\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(random_state=random_seed)\n",
    "\n",
    "    weights_train = np.array([weight_for_0 if label == 0 else weight_for_1 for label in y_train])\n",
    "\n",
    "    model.fit(x_train_scale, y_train, sample_weight=weights_train)\n",
    "\n",
    "    prob_val = model.predict_proba(x_val_scale)[:, 1]\n",
    "    con_matrix_val, outcome, pred = evaluate_model(prob_val, y_val, threshold)\n",
    "\n",
    "    perf_list.append(outcome).\n",
    "\n",
    "\n",
    "feature_select = pd.concat(perf_list).reset_index(drop=True)\n",
    "# Track which feature was removed in each iteration (none removed in the first step)\n",
    "feature_select['feature_removed'] = [''] + feature_sel[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retraining Using Best N Features Identified via Feature Selection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of remaining features based on previous feature selection\n",
    "best_number = 40  # remove n features\n",
    "\n",
    "# Select a specific fold to reproduce best split\n",
    "i = 3\n",
    "print(var)\n",
    "\n",
    "# Define target and features\n",
    "y = data_train['Label']\n",
    "x = data_train.loc[:, chosen_col]\n",
    "\n",
    "# Load saved train/validation split indices\n",
    "train_index = load(f'{cache_dir}\\\\train_index_{i}.joblib')\n",
    "val_index = load(f'{cache_dir}\\\\val_index_{i}.joblib')\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "# Retrieve full feature list and sorted importance ranking\n",
    "features = x.columns.tolist()\n",
    "feature_sel = [features[i] for i in indices]  # features sorted by importance\n",
    "\n",
    "# Get the best feature subset from the selected point onward\n",
    "removed_feature = feature_select.loc[best_number, 'feature_removed']\n",
    "best_features = feature_sel[feature_sel.index(removed_feature) + 1:]\n",
    "print(best_features)\n",
    "\n",
    "# Prepare feature-reduced training and validation sets\n",
    "x_train_best = x_train[best_features]\n",
    "x_val_best = x_val[best_features]\n",
    "\n",
    "# Impute and scale training data\n",
    "x_train_imp, m_mean = imputer(x_train_best, impute_method, train=True)\n",
    "x_train_scale, train_scaler = scaling(x_train_imp, None, train=True)\n",
    "\n",
    "# Impute and scale validation data\n",
    "x_val_imp = imputer(x_val_best, impute_method, False, m_mean)\n",
    "x_val_scale, _ = scaling(x_val_imp, train_scaler, train=False)\n",
    "\n",
    "# Train model \n",
    "model = RandomForestClassifier(random_state=random_seed)\n",
    "weights_train = np.array([weight_for_0 if label == 0 else weight_for_1 for label in y_train])\n",
    "model.fit(x_train_scale, y_train, sample_weight=weights_train)\n",
    "prob_val = model.predict_proba(x_val_scale)[:, 1]\n",
    "\n",
    "# Evaluate model performance\n",
    "con_matrix_val, outcome, pred = evaluate_model(prob_val, y_val, threshold)\n",
    "print(f'test_set_outcome = {outcome}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest Hyperparameter Tuning using RandomizedSearchCV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Random Forest tuning\n",
    "param_distributions = {\n",
    "    'model__n_estimators': randint(50, 200),               # Number of trees in the forest\n",
    "    'model__max_depth': randint(5, 20),                    # Maximum depth of each tree\n",
    "    'model__min_samples_split': randint(2, 10),            # Min samples required to split a node\n",
    "    'model__min_samples_leaf': randint(1, 5),              # Min samples required at a leaf node\n",
    "    'model__criterion': ['gini', 'entropy'],               # Splitting criteria\n",
    "    'model__max_features': ['sqrt', 'log2', None]          # Max features to consider at each split\n",
    "}\n",
    "\n",
    "# Define custom scoring metrics\n",
    "scoring = {\n",
    "    'ROC_AUC': make_scorer(roc_auc_score, needs_proba=True),\n",
    "    'F1_Score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Use best-selected features for training\n",
    "x_train_best = x_train[best_features]\n",
    "\n",
    "# Create a pipeline: imputation -> scaling -> model\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('model', RandomForestClassifier(random_state=random_seed))\n",
    "])\n",
    "\n",
    "# Randomized search with 5-fold cross-validation\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=2000,                      # Number of parameter settings to sample\n",
    "    cv=5,                             # 5-fold CV\n",
    "    scoring=scoring,                 \n",
    "    refit='ROC_AUC',                 # Use AUROC as the primary metric for refitting\n",
    "    verbose=4,\n",
    "    random_state=random_seed,\n",
    "    n_jobs=None                       \n",
    ")\n",
    "\n",
    "# Fit the search object\n",
    "start_time = time.time()\n",
    "search.fit(x_train_best, y_train, model__sample_weight=weights_train)\n",
    "\n",
    "# Compute training time in hours\n",
    "end_time = time.time()\n",
    "total_time_seconds = end_time - start_time\n",
    "total_time_hours = total_time_seconds / 3600\n",
    "print(f\"Total execution time: {total_time_hours:.2f} hours\")\n",
    "\n",
    "# Output best performance and parameters\n",
    "print(\"Best AUROC Score:\", search.best_score_)\n",
    "print(\"Best Parameters:\", search.best_params_)\n",
    "print(\"Best Estimator:\", search.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Random Forest Model Training and Evaluation Across All Cohorts**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train['Label']\n",
    "y_test= data_test['Label']\n",
    "y_ANH= data_ANH['Label']\n",
    "y_WMH= data_WMH['Label']\n",
    "\n",
    "x = data_train.loc[:, chosen_col]  \n",
    "x_test= data_test.loc[:, chosen_col]\n",
    "x_ANH= data_ANH.loc[:, chosen_col]\n",
    "x_WMH= data_WMH.loc[:, chosen_col]\n",
    "\n",
    "# Load saved train/validation split indices\n",
    "train_index = load(f'{cache_dir}\\\\train_index_{i}.joblib')\n",
    "val_index = load(f'{cache_dir}\\\\val_index_{i}.joblib')\n",
    "\n",
    "# Split dataset\n",
    "x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "y_train, y_val = y.iloc[train_index], y.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@SD-LMALS-LY', '@SD-MALS-NE', '@SD-C-NE', '@MN-LMALS-EO', '@MN-LALS-MO', '@MN-UMALS-EO', '@MN-UMALS-NE', '@MN-MALS-NE', '@SD-LALS-EO', '@SD-V-EO', '@SD-UMALS-LY', '@SD-LMALS-EO', 'MO', 'MO#', 'BA#', '@SD-UMALS-NE', '@SD-V-NE', 'RBC', 'NE', '@SD-C-MO', '@SD-AL2-LY', 'NLR', '@WDOP', '@MN-V-LY', '@PDW', '@SD-AL2-MO', 'EO#', '@MN-V-MO', 'PLR', 'PLT', '@MN-V-EO', 'Gender', '@SD-V-LY', 'WBC', '@PCT', 'MCHC', '@SD-V-MO', '@WNOP', 'HCT', '@MAF', 'RDW', 'MCH', 'HGB', '@LHD', 'NRBC', 'MCV', 'Age']\n",
      "\n",
      "===== Developing cohort =====\n",
      " AUROC  AUPRC  accuracy  F1_score  recall  specificity  precision   NPV\n",
      " 0.947  0.832     0.913     0.756   0.717        0.958      0.799 0.936\n",
      "Brier Score: 0.067\n",
      "\n",
      "===== Internal validation cohort =====\n",
      " AUROC  AUPRC  accuracy  F1_score  recall  specificity  precision   NPV\n",
      "  0.95  0.835      0.92     0.754   0.762         0.95      0.746 0.954\n",
      "Brier Score: 0.061\n",
      "\n",
      "===== ANH cohort =====\n",
      " AUROC  AUPRC  accuracy  F1_score  recall  specificity  precision   NPV\n",
      " 0.948  0.832     0.897     0.744   0.777        0.926      0.714 0.946\n",
      "Brier Score: 0.075\n",
      "\n",
      "===== WMH cohort =====\n",
      " AUROC  AUPRC  accuracy  F1_score  recall  specificity  precision   NPV\n",
      " 0.947  0.867     0.916      0.78   0.755        0.956      0.807 0.941\n",
      "Brier Score: 0.064\n"
     ]
    }
   ],
   "source": [
    "best_number = 40 # removed feature numbers\n",
    "fold = 3 # Best split\n",
    "\n",
    "features = x.columns.tolist()\n",
    "feature_sel = [features[i] for i in indices]\n",
    "best_features = feature_sel[feature_sel.index(feature_select.loc[best_number,'feature_removed'])+1:] \n",
    "\n",
    "print (best_features)\n",
    "n = len(best_features)\n",
    "\n",
    "# Choose features with best performance\n",
    "x_train_best = x_train[best_features]\n",
    "x_val_best = x_val [best_features]\n",
    "x_test_best = x_test [best_features]\n",
    "x_ANH_best = x_ANH [best_features]\n",
    "x_WMH_best = x_WMH [best_features]\n",
    "\n",
    "# Developing set\n",
    "x_train_imp, m_mean = imputer(x_train_best, impute_method, train=True)\n",
    "x_train_scale, train_scaler = scaling(x_train_imp, None, train=True)\n",
    "\n",
    "x_val_imp = imputer(x_val_best, impute_method, False, m_mean)\n",
    "x_val_scale, train_scaler = scaling(x_val_imp, train_scaler, train=False)\n",
    "\n",
    "# Internal validation set\n",
    "x_test_imp = imputer(x_test_best, impute_method, False, m_mean)\n",
    "x_test_scale, train_scaler = scaling(x_test_imp, train_scaler, train=False)\n",
    "\n",
    "# ANH dataset\n",
    "x_ANH_imp = imputer(x_ANH_best, impute_method, False, m_mean)\n",
    "x_ANH_scale, train_scaler = scaling(x_ANH_imp, train_scaler, train=False)\n",
    "\n",
    "# WMH dataset\n",
    "x_WMH_imp = imputer(x_WMH_best, impute_method, False, m_mean)\n",
    "x_WMH_scale, train_scaler = scaling(x_WMH_imp, train_scaler, train=False)\n",
    "\n",
    "model = RandomForestClassifier(random_state=random_seed, min_samples_leaf=2, min_samples_split=2,\n",
    "                               n_estimators=196, max_depth=16, criterion='entropy')\n",
    "\n",
    "weights_train = np.array([weight_for_0 if label == 0 else weight_for_1 for label in y_train])\n",
    "\n",
    "model.fit (x_train_scale, y_train, sample_weight=weights_train)\n",
    "prob_val = model.predict_proba(x_val_scale)[:, 1]\n",
    "prob_test = model.predict_proba(x_test_scale)[:, 1]\n",
    "prob_ANH = model.predict_proba(x_ANH_scale)[:, 1]\n",
    "prob_WMH = model.predict_proba(x_WMH_scale)[:, 1]\n",
    "\n",
    "con_matrix_val, outcome, pred =evaluate_model ( prob_val, y_val, threshold)\n",
    "con_matrix_test, outcome_test, pred_test =evaluate_model ( prob_test, y_test, threshold)\n",
    "con_matrix_ANH, outcome_ANH, pred_AN =evaluate_model ( prob_ANH, y_ANH, threshold)\n",
    "con_matrix_WMH, outcome_WMH, pred_WMH =evaluate_model ( prob_WMH, y_WMH, threshold)\n",
    "\n",
    "# Store performance DataFrames\n",
    "cohort_results = {\n",
    "    'Developing cohort': outcome,\n",
    "    'Internal validation cohort': outcome_test,\n",
    "    'ANH cohort': outcome_ANH,\n",
    "    'WMH cohort': outcome_WMH\n",
    "}\n",
    "\n",
    "# Calculate Brier score\n",
    "brier_scores = {\n",
    "    'Developing cohort': brier_score_loss(y_val, prob_val),\n",
    "    'Internal validation cohort': brier_score_loss(y_test, prob_test),\n",
    "    'ANH cohort': brier_score_loss(y_ANH, prob_ANH),\n",
    "    'WMH cohort': brier_score_loss(y_WMH, prob_WMH)\n",
    "}\n",
    "\n",
    "# Print both performance metrics and Brier Score\n",
    "for cohort_name in cohort_results:\n",
    "    print(f'\\n===== {cohort_name} =====')\n",
    "    print(cohort_results[cohort_name].round(3).to_string(index=False))\n",
    "    print(f'Brier Score: {brier_scores[cohort_name]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDA_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
